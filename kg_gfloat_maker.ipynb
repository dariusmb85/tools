{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9045a79a",
   "metadata": {},
   "source": [
    "To begin we start with importing the necessary modules.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa99ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n",
      "SQLalchemy is not installed. No support for SQL output.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.graph import KnowledgeGraph\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import gzip\n",
    "from simpledbf import Dbf5\n",
    "\n",
    "gis = GIS(\"https://arcgis.edc.renci.org/portal\",'dariusmb',password='dmbo9889')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bde8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gis.content.create_service(\n",
    "    name=\"Update\",\n",
    "    capabilities=\"Query,Editing,Create,Update,Delete\",\n",
    "    service_type=\"KnowledgeGraph\",\n",
    ")\n",
    "#kg = KnowledgeGraph(\"https://arcgis.edc.renci.org/server/rest/services/Hosted/2_6_KG/KnowledgeGraphServer\",gis = gis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af1e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = KnowledgeGraph(\"https://arcgis.edc.renci.org/portal/rest/services/Hosted/Update/KnowledgeGraphServer\", gis=gis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10da66",
   "metadata": {},
   "source": [
    "This is a logic test to see that your KG is made and empty. Should only have Document and HasDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255f9d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document\n",
      "HasDocument\n"
     ]
    }
   ],
   "source": [
    "for types in kg.datamodel['entity_types']:\n",
    "    print(types)\n",
    "for types in kg.datamodel['relationship_types']:\n",
    "    print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ee7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_headers(file_path):\n",
    "    if file_path.endswith('.gz'):\n",
    "        with gzip.open(file_path, 'rt') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            headers = next(reader)  # Read the first row (headers)\n",
    "    else:\n",
    "        with open(file_path, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            headers = next(reader)  # Read the first row (headers)\n",
    "    # Read the first few rows to infer data types using pandas\n",
    "    sample_data = pd.read_csv(file_path, nrows=5)\n",
    "    dtypes = sample_data.dtypes.tolist()\n",
    "\n",
    "    return ['label'] + headers, dtypes\n",
    "\n",
    "def build_entity_types(entity_properties):\n",
    "    entity_types = []\n",
    "    \n",
    "    for entity, properties in entity_properties.items():\n",
    "        entity_dict = {\n",
    "            \"name\": entity,\n",
    "            \"alias\": entity,\n",
    "            \"role\": \"esriGraphNamedObjectRegular\",\n",
    "            \"strict\": False,\n",
    "            \"properties\": {}\n",
    "        }\n",
    "        \n",
    "        for prop in properties:\n",
    "            prop_dict = {\n",
    "                \"name\": prop,\n",
    "                \"alias\": prop,\n",
    "                \"fieldType\": \"esriFieldTypeInteger\",\n",
    "                \"editable\": True,\n",
    "                \"visible\": True,\n",
    "                \"required\": False,\n",
    "                \"IsSystemMaintained\": False,\n",
    "                \"role\": \"esriGraphPropertyRegular\"\n",
    "            }\n",
    "            \n",
    "            entity_dict[\"properties\"][prop] = prop_dict\n",
    "        \n",
    "        entity_types.append(entity_dict)\n",
    "    \n",
    "    return entity_types\n",
    "\n",
    "\n",
    "def build_props(name, fieldType):\n",
    "    prop_types = []\n",
    "    prop_dict = {\n",
    "        \"name\": name,\n",
    "        \"alias\": name,\n",
    "        \"fieldType\": fieldType,\n",
    "        \"editable\": True,\n",
    "        \"visible\": True,\n",
    "        \"required\": False,\n",
    "        \"IsSystemMaintained\": False,\n",
    "        \"role\": \"esriGraphPropertyRegular\"\n",
    "    }\n",
    "    prop_types.append(prop_dict)\n",
    "    return prop_types\n",
    "\n",
    "def build_relationship_types(relationships):\n",
    "    relationship_types = []\n",
    "    \n",
    "    for relate in relationships:\n",
    "        relate_dict = {\n",
    "            \"name\": relate,\n",
    "            \"alias\": relate,\n",
    "            \"role\": \"esriGraphNamedObjectRegular\",\n",
    "            \"strict\": False\n",
    "\n",
    "        }\n",
    "        relationship_types.append(relate_dict)\n",
    "    \n",
    "    return relationship_types\n",
    "\n",
    "def build_spatial_props(geo_type):\n",
    "    space_types = []\n",
    "    space_dict = {\n",
    "        \"name\": \"shape\",\n",
    "        \"alias\": \"shape\",\n",
    "        \"fieldType\": \"esriFieldTypeGeometry\",\n",
    "        \"geometryType\": geo_type,\n",
    "        \"hasZ\": False,\n",
    "        \"hasM\": False,\n",
    "        \"editable\": True,\n",
    "        \"visible\": True,\n",
    "        \"required\": False,\n",
    "        \"IsSystemMaintained\": False,\n",
    "        \"role\": \"esriGraphPropertyRegular\"\n",
    "    }\n",
    "    space_types.append(space_dict)\n",
    "    return space_types\n",
    "\n",
    "# Function to add properties to the knowledge graph\n",
    "def add_properties(type_name, properties):\n",
    "    for prop_name, field_type in properties:\n",
    "        prop_types = build_props(prop_name, field_type)\n",
    "        kg.graph_property_adds(type_name=type_name, graph_properties=prop_types)\n",
    "\n",
    "\n",
    "def convert_columns_to_string(dataframe, columns):\n",
    "    for column in columns:\n",
    "        dataframe.loc[:,column] = dataframe[column].astype(str)\n",
    "\n",
    "def handle_nan_values(value, default=\"Unknown\"):\n",
    "    return value if not pd.isna(value) else default\n",
    "\n",
    "def convert_values_to_string(*values):\n",
    "    return [handle_nan_values(value) for value in values]\n",
    "\n",
    "def add_entities_in_batches(entity_dicts, batch_size=100):\n",
    "    \"\"\"\n",
    "    Add entities to the knowledge graph in batches.\n",
    "\n",
    "    Parameters:\n",
    "    - entity_dicts: List of entity dictionaries to be added to the knowledge graph.\n",
    "    - batch_size: Size of each batch for adding entities. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    - List of results for each batch addition.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    return_dict = []\n",
    "    num_batches = (len(entity_dicts) + batch_size - 1) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(entity_dicts))\n",
    "        batch = entity_dicts[start_idx:end_idx]\n",
    "        \n",
    "        #return_dict = []\n",
    "        for j, entity_dict in enumerate(batch):\n",
    "            entity_dict[\"_index\"] = start_idx + j  # Assign index based on position in batch\n",
    "            return_dict.append(entity_dict)\n",
    "\n",
    "        result = kg.apply_edits(adds=batch)\n",
    "        results.append(result)\n",
    "    \n",
    "    return results, return_dict\n",
    "\n",
    "\n",
    "def find_entity_from_result(results, index, entity_type, batch_size=50000):\n",
    "    # Calculate the batch index and index within the batch\n",
    "    batch_index = index // batch_size\n",
    "    index_within_batch = index % batch_size\n",
    "    \n",
    "    uuid = results_h[batch_index][\"editsResult\"][entity_type][\"addResults\"][index_within_batch][\"id\"]\n",
    "    return uuid\n",
    "\n",
    "def search_edits_h_by_hh_id(edits, target_id, type_id):\n",
    "    \"\"\"\n",
    "    Search for a household entity in edits_h based on the hh_id.\n",
    "\n",
    "    Parameters:\n",
    "    - edits_h: List of household entities.\n",
    "    - target_hh_id: The hh_id to search for.\n",
    "\n",
    "    Returns:\n",
    "    - Index of the matching household entity.\n",
    "    - Matching household entity.\n",
    "    \"\"\"\n",
    "    for index, entity in enumerate(edits):\n",
    "        hh_id_h = entity['_properties'][type_id]\n",
    "        if hh_id_h == target_id:\n",
    "            return index, entity\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def add_relates_in_batch(relate_dict, batch_size):\n",
    "    relate = []\n",
    "    num_batches = (len(relate_dict) + batch_size - 1) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(relate_dict))\n",
    "        batch = relate_dict[start_idx:end_idx]\n",
    "        \n",
    "        result = kg.apply_edits(adds=batch)\n",
    "        relate.append(result)\n",
    "        \n",
    "    return relate\n",
    "\n",
    "def create_index_dict(edits, type_id):\n",
    "    \"\"\"\n",
    "    Create a dictionary for fast lookup of household entities by hh_id.\n",
    "\n",
    "    Parameters:\n",
    "    - edits_h: List of household entities.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary where keys are hh_id values and values are lists of indices.\n",
    "    \"\"\"\n",
    "    index_dict = {}\n",
    "    for index, entity in enumerate(edits):\n",
    "        hh_id_h = entity['_properties'][type_id]\n",
    "        if hh_id_h not in index_dict:\n",
    "            index_dict[hh_id_h] = []\n",
    "        index_dict[hh_id_h].append(index)\n",
    "    return index_dict\n",
    "\n",
    "def get_field_type(dtype):\n",
    "    \"\"\"\n",
    "    Map pandas dtype to esriFieldType.\n",
    "    \"\"\"\n",
    "    if dtype == 'int64':\n",
    "        return \"esriFieldTypeInteger\"\n",
    "    elif dtype == 'float64':\n",
    "        return \"esriFieldTypeDouble\"\n",
    "    elif dtype == 'object':\n",
    "        return \"esriFieldTypeString\"\n",
    "    # Add more mappings as needed for other data types\n",
    "    else:\n",
    "        return \"esriFieldTypeString\"  # Default to string if unknown dtype\n",
    "\n",
    "def check_column_name(column_name):\n",
    "    \"\"\"\n",
    "    Replace spaces in a column name with underscores.\n",
    "\n",
    "    Parameters:\n",
    "    - column_name: String representing the column name.\n",
    "\n",
    "    Returns:\n",
    "    - String with spaces replaced by underscores.\n",
    "    \"\"\"\n",
    "    return column_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "def add_properties_batch(entity_name, df):\n",
    "    ent_prop = [(check_column_name(column), get_field_type(dtype)) for column, dtype in zip(df.columns, df.dtypes)]\n",
    "    add_properties(entity_name, ent_prop)\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Clean column names of a DataFrame by replacing spaces with underscores.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame whose column names need to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with cleaned column names.\n",
    "    \"\"\"\n",
    "    cleaned_columns = [check_column_name(column) for column in df.columns]\n",
    "    df.columns = cleaned_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76fb5817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entityAddResults': [{'name': 'Household'}, {'name': 'Person'}, {'name': 'Workplace'}, {'name': 'Gages'}, {'name': 'Matthew_Flood_Impact'}, {'name': 'Priv_School'}, {'name': 'Pub_School'}, {'name': 'Florence_Flood_Impact'}], 'relationshipAddResults': [{'name': 'LivesIn'}, {'name': 'Holds'}, {'name': 'Within'}, {'name': 'EvacPath'}, {'name': 'WorksAt'}, {'name': 'Attends'}]}\n"
     ]
    }
   ],
   "source": [
    "entity_properties = {\n",
    "    \"Household\": [\"label\", \"logrecno\", \"hh_age\", \"hh_income\", \"hh_race\", \"ethnicity\",\n",
    "                  \"size\", \"state_fips\", \"county_fips\", \"tract_fips\", \"blkgrp_fips\",\n",
    "                  \"puma_fips\",\"evelation\"],\n",
    "    \"Person\": [\"label\", \"p_id\", \"sporder\", \"relshipp\", \"rac1p\", \"agep\",\n",
    "               \"sex\", \"hisp\"],\n",
    "    \"Workplace\": [\"label\"],\n",
    "    \"Gages\": ['label','DOT_Div'],\n",
    "    \"Matthew_Flood_Impact\": [\"label\",\"index\",\"object_id\"],\n",
    "    \"Florence_Flood_Impact\": [\"label\",\"index\",\"object_id\"],\n",
    "    \"Pub_School\": [\"label\",\"fips\"],\n",
    "    \"Priv_School\": [\"label\",\"COUNTY_FIPS\",\"fips\"]\n",
    "}\n",
    "\n",
    "entity_types = build_entity_types(entity_properties)\n",
    "\n",
    "relationships = {\n",
    "    \"LivesIn\", \"Attends\",\"WorksAt\",\"Holds\", \"EvacPath\", \"Within\"\n",
    "}\n",
    "relate_types = build_relationship_types(relationships)\n",
    "\n",
    "res = kg.named_object_type_adds(entity_types, relate_types)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e2a0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propertyAddResults': [{'name': 'shape'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#esriGeometryTypePoint\n",
    "#esriGeometryTypePolyline\n",
    "#esriGeometryTypePolygon\n",
    "#esriGeometryTypeMultipoint\n",
    "spatial_point = build_spatial_props(\"esriGeometryPoint\")\n",
    "\n",
    "spatial_point\n",
    "kg.graph_property_adds(type_name='Household', graph_properties=spatial_point)\n",
    "kg.graph_property_adds(type_name='Workplace', graph_properties=spatial_point)\n",
    "kg.graph_property_adds(type_name='Gages', graph_properties=spatial_point)\n",
    "kg.graph_property_adds(type_name='Pub_School', graph_properties=spatial_point)\n",
    "kg.graph_property_adds(type_name='Priv_School', graph_properties=spatial_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a13c6",
   "metadata": {},
   "source": [
    "Below is a WIP for adding polygon shapes into knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aada8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial_poly = build_spatial_props(\"esriGeometryTypePolygon\")\n",
    "\n",
    "#kg.graph_property_adds(type_name='Matthew_Flood_Impact', graph_properties=spatial_poly)\n",
    "#kg.graph_property_adds(type_name='Florence_Flood_Impact', graph_properties=spatial_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c1b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of properties for each type\n",
    "person_properties = [\n",
    "    (\"hh_id\", \"esriFieldTypeString\"),\n",
    "    (\"wp_id\", \"esriFieldTypeString\"),\n",
    "    (\"job_income_bucket\", \"esriFieldTypeString\"),\n",
    "    (\"school_id\", \"esriFieldTypeString\"),\n",
    "    (\"school_type\", \"esriFieldTypeString\"),\n",
    "    (\"grade_lvl\", \"esriFieldTypeString\"),\n",
    "    (\"serlino\", \"esriFieldTypeString\"),\n",
    "]\n",
    "\n",
    "workplace_properties = [\n",
    "    (\"x\", \"esriFieldTypeDouble\"),\n",
    "    (\"y\", \"esriFieldTypeDouble\"),\n",
    "    (\"wp_id\", \"esriFieldTypeString\"),\n",
    "    (\"NAICS\", \"esriFieldTypeString\"),\n",
    "    (\"blkgrp_fips_wp\", \"esriFieldTypeDouble\"),\n",
    "    (\"elevation\", \"esriFieldTypeDouble\"),\n",
    "]\n",
    "\n",
    "household_properties = [\n",
    "    (\"x\", \"esriFieldTypeDouble\"),\n",
    "    (\"y\", \"esriFieldTypeDouble\"),\n",
    "    (\"hh_id\", \"esriFieldTypeString\"),\n",
    "    (\"serlino\", \"esriFieldTypeString\"),\n",
    "]\n",
    "\n",
    "gage_properties = [\n",
    "    (\"name\", \"esriFieldTypeString\"),\n",
    "    (\"matthew_peak\", \"esriFieldTypeDouble\"),\n",
    "    (\"florence_peak\", \"esriFieldTypeDouble\"),\n",
    "    (\"site_id\", \"esriFieldTypeString\")\n",
    "]\n",
    "\n",
    "matt_properties = [\n",
    "    ('SITE_ID', 'esriFieldTypeString'),\n",
    "    ('USER_FLAG', 'esriFieldTypeDouble'),\n",
    "    ('Shape_Leng', 'esriFieldTypeDouble'),\n",
    "    ('Shape_Area', 'esriFieldTypeDouble'),\n",
    "    ('LEVEL_ID', 'esriFieldTypeString'),\n",
    "    ('JOIN_ID', 'esriFieldTypeString'),\n",
    "    ('OBJECTID', 'esriFieldTypeInteger')\n",
    "]\n",
    "\n",
    "flor_properties = [\n",
    "    ('SITE_ID', 'esriFieldTypeString'),\n",
    "    ('USER_FLAG', 'esriFieldTypeDouble'),\n",
    "    ('Shape_Leng', 'esriFieldTypeDouble'),\n",
    "    ('Shape_Area', 'esriFieldTypeDouble'),\n",
    "    ('LEVEL_ID', 'esriFieldTypeString'),\n",
    "    ('JOIN_ID', 'esriFieldTypeString'),\n",
    "    ('OBJECTID', 'esriFieldTypeInteger'),\n",
    "]\n",
    "\n",
    "pub_properties = [\n",
    "    ('NCESSCH','esriFieldTypeString'),\n",
    "    ('NAME', 'esriFieldTypeString'),\n",
    "    ('STREET', 'esriFieldTypeString'),\n",
    "    ('CITY', 'esriFieldTypeString'),\n",
    "    ('STATE', 'esriFieldTypeString'),\n",
    "    ('ZIP', 'esriFieldTypeDouble'),\n",
    "    ('STFIP', 'esriFieldTypeDouble'),\n",
    "    ('CNTY', 'esriFieldTypeDouble'),\n",
    "]\n",
    "\n",
    "priv_properties = [\n",
    "    ('SCHOOL_NAME', 'esriFieldTypeString'),\n",
    "    ('CITY', 'esriFieldTypeString'),\n",
    "    ('STATE', 'esriFieldTypeString'),\n",
    "    ('COUNTY_FIPS', 'esriFieldTypeInteger'),\n",
    "    ('COUNTY_NAME', 'esriFieldTypeString'),\n",
    "    ('PPIN', 'esriFieldTypeString'),\n",
    "]\n",
    "# Add properties for 'Person'\n",
    "add_properties(\"Person\", person_properties)\n",
    "# Add properties for 'Workplace' and 'Household'\n",
    "add_properties(\"Workplace\", workplace_properties)\n",
    "add_properties(\"Household\", household_properties)\n",
    "add_properties(\"Gages\", gage_properties)\n",
    "add_properties(\"Florence_Flood_Impact\", flor_properties)\n",
    "add_properties(\"Matthew_Flood_Impact\", matt_properties)\n",
    "add_properties(\"Pub_School\", pub_properties)\n",
    "add_properties(\"Priv_School\", priv_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5510dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hh = pd.read_csv(\"2019_ver1_37/37/NC2019_Households.csv\")\n",
    "df_Person = pd.read_csv(\"2019_ver1_37/37/NC2019_Persons.csv.gz\")\n",
    "df_Work = pd.read_csv(\"2019_ver1_37/37/NC2019_Workplaces.csv.gz\")\n",
    "df_pub = pd.read_csv(\"2019_ver1_37/37/NC_schools_pub.csv.gz\")\n",
    "df_priv= pd.read_csv(\"2019_ver1_37/37/NC_schools_priv.csv.gz\")\n",
    "\n",
    "# Replace 'your_file.dbf' with the actual path to your .dbf file\n",
    "dbf_path = 'coastalGages.dbf'\n",
    "# Use simpledbf to read the .dbf file\n",
    "dbf = Dbf5(dbf_path)\n",
    "# Convert the .dbf file to a DataFrame\n",
    "gages = dbf.to_dataframe()\n",
    "matt_sdf = pd.DataFrame.spatial.from_featureclass('matthew.shp')\n",
    "flor_sdf = pd.DataFrame.spatial.from_featureclass('florence.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0100970",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_list = [37129, 37065, 37019, 37155, 37139]\n",
    "df_hh_subset = df_hh[(df_hh['county_fips'].isin(county_list))]\n",
    "hh_subset_id_list = df_hh_subset['hh_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3da9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = ['serialno']\n",
    "convert_columns_to_string(df_hh_subset, columns_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac03ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person_subset = df_Person[(df_Person['hh_id'].isin(hh_subset_id_list))]\n",
    "workplace_id_list = df_Person['workplace_id'].unique()\n",
    "columns_to_convert = ['school_id', 'school_type', 'grade_level', 'job_income_bucket', 'serialno']\n",
    "convert_columns_to_string(df_person_subset, columns_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad715362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work_subset = df_Work[df_Work['workplace_id'].isin(workplace_id_list)]\n",
    "columns_to_convert = ['workplace_id']\n",
    "convert_columns_to_string(df_work_subset, columns_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaaf5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_priv = df_priv[(df_priv['COUNTY_FIPS'].isin([129, 65, 19, 155, 139]))]\n",
    "df_pub = df_pub[(df_pub['CNTY'].isin([37129, 37065, 37019, 37155, 37139]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "456081af",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert=['NCESSCH']\n",
    "convert_columns_to_string(df_pub, columns_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01238bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179     370004002349\n",
       "241     370010202506\n",
       "247     370010802534\n",
       "291     370014102927\n",
       "358     370032303256\n",
       "            ...     \n",
       "2018    370393002243\n",
       "2019    370393002244\n",
       "2020    370393002245\n",
       "2021    370393002246\n",
       "2022    370393002247\n",
       "Name: NCESSCH, Length: 139, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pub['NCESSCH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10f32ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual df ingest\n",
    "# Initialize lists to store entity dictionaries\n",
    "edits_p = []\n",
    "edits_h = []\n",
    "edits_w = []\n",
    "\n",
    "# Iterate over df_Person_subset to build entity dictionaries for persons\n",
    "for per in df_person_subset.itertuples():\n",
    "    # Construct person_edit dictionary\n",
    "    school_id, school_type, grade_lvl, income, serialno, workplace_id = convert_values_to_string(\n",
    "    per.school_id, per.school_type, per.grade_level, per.job_income_bucket, per.serialno, per.workplace_id)\n",
    "    person_edit = {\n",
    "            \"_objectType\": \"entity\",\n",
    "            \"_typeName\": \"Person\",\n",
    "            \"_properties\": {\n",
    "                \"hh_id\": per.hh_id,\n",
    "                \"sex\": per.sex,\n",
    "                \"agep\": per.agep,\n",
    "                \"rac1p\": per.rac1p,\n",
    "                \"wp_id\": workplace_id,\n",
    "                \"hisp\": per.hisp,\n",
    "                \"school_id\": school_id,\n",
    "                \"school_type\": school_type,\n",
    "                \"grade_lvl\": grade_lvl,\n",
    "                \"job_income_bucket\": income,\n",
    "                \"relshipp\": per.relshipp,\n",
    "                \"sporder\": per.sporder,\n",
    "                \"serlino\": serialno,\n",
    "                \"p_id\": per.person_id_numeric\n",
    "            }\n",
    "    }\n",
    "\n",
    "    # Add person_edit dictionary to edits_ph list\n",
    "    edits_p.append(person_edit)\n",
    "\n",
    "# Now, you have populated edits_ph list with entity dictionaries for persons.\n",
    "\n",
    "# Iterate over df_Work_subset to build entity dictionaries for workplaces\n",
    "for work in df_work_subset.itertuples():\n",
    "    workplace_id = convert_values_to_string(work.workplace_id)\n",
    "    # Construct workplace_edit dictionary\n",
    "    workplace_edit = {\n",
    "                \"_objectType\": \"entity\",\n",
    "                \"_typeName\": \"Workplace\",\n",
    "                \"_properties\": {\n",
    "                    \"NAICS\": work.NAICS,\n",
    "                    \"blkgrp_fips_wp\": work.blkgrp_fips_workplace,\n",
    "                    \"elevation\": work.elevation,\n",
    "                    \"wp_id\": work.workplace_id,\n",
    "                    \"shape\": {\n",
    "                        'x': float(work.lon_workplace),\n",
    "                        'y': float(work.lat_workplace),\n",
    "                        '_objectType': 'geometry'\n",
    "                    }\n",
    "                }\n",
    "    }\n",
    "    edits_w.append(workplace_edit)\n",
    "\n",
    "# Now, you have populated edits_phw list with entity dictionaries for workplaces.\n",
    "\n",
    "# Iterate over df_hh_subset to build entity dictionaries for households\n",
    "for hh in df_hh_subset.itertuples():\n",
    "    serialno = convert_values_to_string(hh.serialno)\n",
    "    # Construct house_edit dictionary\n",
    "    house_edit = {\n",
    "            \"_objectType\": \"entity\",\n",
    "            \"_typeName\": \"Household\",\n",
    "            \"_properties\": {\n",
    "                \"hh_age\": hh.hh_age,\n",
    "                \"size\": hh.size,\n",
    "                \"hh_race\": hh.hh_race,\n",
    "                \"ethnicity\": hh.ethnicity,\n",
    "                \"hh_income\": hh.hh_income, \n",
    "                \"state_fips\": hh.state_fips,\n",
    "                \"county_fips\": hh.county_fips,\n",
    "                \"tract_fips\": hh.tract_fips,\n",
    "                \"blkgrp_fips\": hh.blkgrp_fips,\n",
    "                \"evelation\": hh.elevation,\n",
    "                \"serlino\": hh.serialno,\n",
    "                \"hh_id\": hh.hh_id,\n",
    "                \"shape\": {\n",
    "                    'x': float(hh.LON),\n",
    "                    'y': float(hh.LAT),\n",
    "                    '_objectType': 'geometry'\n",
    "                }\n",
    "            }\n",
    "    }\n",
    "    edits_h.append(house_edit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32365d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_pub = []\n",
    "for pub in df_pub.itertuples():\n",
    "    ncessch = convert_values_to_string(pub.NCESSCH)\n",
    "    pub_edit = {\n",
    "        \"_objectType\": \"entity\",\n",
    "        \"_typeName\": \"Pub_School\",\n",
    "        \"_properties\":{\n",
    "            \"CNTY\": pub.CNTY,\n",
    "            \"STFIP\": pub.STFIP,\n",
    "            \"ZIP\": pub.ZIP,\n",
    "            \"STATE\": pub.STATE,\n",
    "            \"CITY\": pub.CITY,\n",
    "            \"STREET\": pub.STREET,\n",
    "            \"NAME\": pub.NAME,\n",
    "            \"fips\": pub.fips,\n",
    "            \"NCESSCH\": pub.NCESSCH,\n",
    "            \"shape\":{\n",
    "                'x': float(pub.LON),\n",
    "                'y': float(pub.LAT),\n",
    "                '_objectType': 'geometry'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    edits_pub.append(pub_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00304a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_priv= []\n",
    "for priv in df_priv.itertuples():\n",
    "    priv_edit = {\n",
    "        \"_objectType\": \"entity\",\n",
    "        \"_typeName\": \"Priv_School\",\n",
    "        \"_properties\":{\n",
    "            \"COUNTY_NAME\": priv.COUNTY_NAME,\n",
    "            \"COUNTY_FIPS\": priv.COUNTY_FIPS,\n",
    "            \"fips\": priv.fips,\n",
    "            \"STATE\": priv.STATE,\n",
    "            \"CITY\": priv.CITY,\n",
    "            \"SCHOOL_NAME\": priv.SCHOOL_NAME,\n",
    "            \"PPIN\": priv.PPIN,\n",
    "            \"shape\":{\n",
    "                'x': float(pub.LON),\n",
    "                'y': float(pub.LAT),\n",
    "                '_objectType': 'geometry'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    edits_priv.append(priv_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5480885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_g = []\n",
    "for gage in gages.itertuples():\n",
    "    #print(gage)\n",
    "    gage_edit = {\n",
    "        \"_objectType\": \"entity\",\n",
    "        \"_typeName\": \"Gages\",\n",
    "        \"_properties\":{\n",
    "            'DOT_Div': gage.DOT_Divisi,\n",
    "            'matthew_peak': gage.MATTHEW_PE,\n",
    "            'florence_peak': gage.FLORENCE_P,\n",
    "            'name': gage.NAME,\n",
    "            'site_id': gage.SITE_ID,\n",
    "            'shape':{\n",
    "                'x': float(gage.LONGITUDE),\n",
    "                'y': float(gage.LATITUDE),\n",
    "                '_objectType': 'geometry'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    edits_g.append(gage_edit)\n",
    "result_g = kg.apply_edits(adds=edits_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b2d245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 87.81038975715637 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "results_h, edits_h_id = add_entities_in_batches(edits_h, 50000)\n",
    "end_time = time.time()\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7975e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 206.46732091903687 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results_p, edits_p_id = add_entities_in_batches(edits_p, 50000)\n",
    "end_time = time.time()\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98d87e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 113.09378552436829 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results_w, edits_w_id = add_entities_in_batches(edits_w, 50000)\n",
    "end_time = time.time()\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27cac4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.34399938583374023 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results_pub, edits_pub_id = add_entities_in_batches(edits_pub, 10000)\n",
    "end_time = time.time()\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3979a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.11199259757995605 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results_priv, edits_priv_id = add_entities_in_batches(edits_priv, 10000)\n",
    "end_time = time.time()\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1455d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 3.598994731903076 seconds\n",
      "Adding to knowledge graph\n",
      "Elapsed time: 357.2031171321869 seconds\n",
      "Elapsed time: 1.6330063343048096 seconds\n",
      "Adding to knowledge graph\n",
      "Elapsed time: 142.0149028301239 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Create index dictionary for edits_h_id\n",
    "index_dict_h = create_index_dict(edits_h_id, \"hh_id\")\n",
    "\n",
    "# Process person entities in batches\n",
    "batch_size = 50000\n",
    "person_uuids = []\n",
    "relates_h = []\n",
    "start_idx = 0\n",
    "while start_idx < len(edits_p_id):\n",
    "    end_idx = min(start_idx + batch_size, len(edits_p_id))\n",
    "    batch_person_entities = edits_p_id[start_idx:end_idx]\n",
    "\n",
    "    for person_entity in batch_person_entities:\n",
    "        hh_id = person_entity['_properties']['hh_id']\n",
    "        person_index = person_entity['_index']\n",
    "\n",
    "        # Search for hh_id in index dictionary\n",
    "        matching_indices = index_dict_h.get(hh_id, [])\n",
    "        for matching_index in matching_indices:\n",
    "            # Get UUIDs for household and person entities\n",
    "            household_uuid = results_h[matching_index // 50000]['editsResult']['Household']['addResults'][matching_index % 50000]['id']\n",
    "            person_uuid = results_p[person_index // 50000]['editsResult']['Person']['addResults'][person_index % 50000]['id']\n",
    "            \n",
    "            # Create relationship\n",
    "            relationship = {\n",
    "                \"_objectType\": \"relationship\",\n",
    "                \"_typeName\": \"LivesIn\",\n",
    "                \"_originEntityId\": person_uuid,\n",
    "                \"_destinationEntityId\": household_uuid,\n",
    "                \"_properties\": {}\n",
    "            }\n",
    "            relates_h.append(relationship)\n",
    "\n",
    "    start_idx += batch_size\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "print(\"Adding to knowledge graph\")\n",
    "start_time = time.time()\n",
    "p_h_relates = add_relates_in_batch(relates_h, 50000)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "# Create index dictionary for edits_w_id\n",
    "index_dict_w = create_index_dict(edits_w_id, \"wp_id\")\n",
    "\n",
    "# Process person entities in batches\n",
    "batch_size = 50000\n",
    "person_uuids = []\n",
    "relates_w = []\n",
    "start_idx = 0\n",
    "while start_idx < len(edits_p_id):\n",
    "    end_idx = min(start_idx + batch_size, len(edits_p_id))\n",
    "    batch_person_entities = edits_p_id[start_idx:end_idx]\n",
    "\n",
    "    for person_entity in batch_person_entities:\n",
    "        wp_id = person_entity['_properties']['wp_id']\n",
    "        person_index = person_entity['_index']\n",
    "\n",
    "        # Search for hh_id in index dictionary\n",
    "        matching_indices = index_dict_w.get(wp_id, [])\n",
    "        for matching_index in matching_indices:\n",
    "            # Get UUIDs for household and person entities\n",
    "            workplace_uuid = results_w[matching_index // 50000]['editsResult']['Workplace']['addResults'][matching_index % 50000]['id']\n",
    "            person_uuid = results_p[person_index // 50000]['editsResult']['Person']['addResults'][person_index % 50000]['id']\n",
    "            \n",
    "            # Create relationship\n",
    "            relationship = {\n",
    "                \"_objectType\": \"relationship\",\n",
    "                \"_typeName\": \"WorksAt\",\n",
    "                \"_originEntityId\": person_uuid,\n",
    "                \"_destinationEntityId\": workplace_uuid,\n",
    "                \"_properties\": {}\n",
    "            }\n",
    "            relates_w.append(relationship)\n",
    "\n",
    "    start_idx += batch_size\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "print(\"Adding to knowledge graph\")\n",
    "start_time = time.time()\n",
    "p_w_relates = add_relates_in_batch(relates_w, 50000)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b028453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.4629979133605957 seconds\n",
      "Adding to knowledge graph\n",
      "Elapsed time: 42.99869632720947 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Create index dictionary for edits_w_id\n",
    "index_dict_pub = create_index_dict(edits_pub_id, \"NCESSCH\")\n",
    "\n",
    "# Process person entities in batches\n",
    "batch_size = 50000\n",
    "person_uuids = []\n",
    "relates_pub = []\n",
    "start_idx = 0\n",
    "while start_idx < len(edits_p_id):\n",
    "    end_idx = min(start_idx + batch_size, len(edits_p_id))\n",
    "    batch_person_entities = edits_p_id[start_idx:end_idx]\n",
    "\n",
    "    for person_entity in batch_person_entities:\n",
    "        pub_id = person_entity['_properties']['school_id']\n",
    "        person_index = person_entity['_index']\n",
    "        #print(pub_id, person_index)\n",
    "        if pub_id == 'nan':\n",
    "            #print(pub_id)\n",
    "            continue\n",
    "\n",
    "        # Search for hh_id in index dictionary\n",
    "        matching_indices = index_dict_pub.get(pub_id, [])\n",
    "        for matching_index in matching_indices:\n",
    "            # Get UUIDs for household and person entities\n",
    "            pub_uuid = results_pub[matching_index // 50000]['editsResult']['Pub_School']['addResults'][matching_index % 50000]['id']\n",
    "            person_uuid = results_p[person_index // 50000]['editsResult']['Person']['addResults'][person_index % 50000]['id']\n",
    "            \n",
    "            # Create relationship\n",
    "            relationship = {\n",
    "                \"_objectType\": \"relationship\",\n",
    "                \"_typeName\": \"Attends\",\n",
    "                \"_originEntityId\": person_uuid,\n",
    "                \"_destinationEntityId\": pub_uuid,\n",
    "                \"_properties\": {}\n",
    "            }\n",
    "            relates_pub.append(relationship)\n",
    "\n",
    "    start_idx += batch_size\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "print(\"Adding to knowledge graph\")\n",
    "start_time = time.time()\n",
    "p_pub_relates = add_relates_in_batch(relates_pub, 50000)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dca224cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.27098774909973145 seconds\n",
      "Adding to knowledge graph\n",
      "Elapsed time: 0.7270524501800537 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Create index dictionary for edits_w_id\n",
    "index_dict_priv = create_index_dict(edits_priv_id, \"PPIN\")\n",
    "\n",
    "# Process person entities in batches\n",
    "batch_size = 50000\n",
    "person_uuids = []\n",
    "relates_priv = []\n",
    "start_idx = 0\n",
    "while start_idx < len(edits_p_id):\n",
    "    end_idx = min(start_idx + batch_size, len(edits_p_id))\n",
    "    batch_person_entities = edits_p_id[start_idx:end_idx]\n",
    "\n",
    "    for person_entity in batch_person_entities:\n",
    "        priv_id = person_entity['_properties']['school_id']\n",
    "        person_index = person_entity['_index']\n",
    "        #print(pub_id, person_index)\n",
    "        if priv_id == 'nan':\n",
    "            #print(pub_id)\n",
    "            continue\n",
    "\n",
    "        # Search for hh_id in index dictionary\n",
    "        matching_indices = index_dict_priv.get(priv_id, [])\n",
    "        for matching_index in matching_indices:\n",
    "            # Get UUIDs for household and person entities\n",
    "            priv_uuid = results_priv[matching_index // 50000]['editsResult']['Priv_School']['addResults'][matching_index % 50000]['id']\n",
    "            person_uuid = results_p[person_index // 50000]['editsResult']['Person']['addResults'][person_index % 50000]['id']\n",
    "            \n",
    "            # Create relationship\n",
    "            relationship = {\n",
    "                \"_objectType\": \"relationship\",\n",
    "                \"_typeName\": \"Attends\",\n",
    "                \"_originEntityId\": person_uuid,\n",
    "                \"_destinationEntityId\": priv_uuid,\n",
    "                \"_properties\": {}\n",
    "            }\n",
    "            relates_priv.append(relationship)\n",
    "\n",
    "    start_idx += batch_size\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "print(\"Adding to knowledge graph\")\n",
    "start_time = time.time()\n",
    "p_pub_relates = add_relates_in_batch(relates_priv, 50000)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a6b8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edits_p_id[29675]['_properties']['school_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc65865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edits_p_id[29675]['_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1195165",
   "metadata": {},
   "source": [
    "Below is an example of the mapping of KG on a gis map. There is a limit of 2000 entries here and won't display entire graph. The KG can be access from arcgis Pro gui for full visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dd1863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arcgis.features import FeatureSet, Feature\n",
    "from arcgis.geometry import Geometry\n",
    "\n",
    "query_results = kg.query(\"MATCH (n:Household) RETURN n\")\n",
    "house_list = []\n",
    "\n",
    "for house in query_results:\n",
    "    geom = Geometry(house[0]['_properties']['shape'])\n",
    "    house_feat = Feature(geometry=geom)\n",
    "    house_list.append(house_feat)\n",
    "    \n",
    "house_fs = FeatureSet(features= house_list, geometry_type= 'Point', spatial_reference= kg.datamodel['spatial_reference'])\n",
    "house_sdf = house_fs.sdf\n",
    "\n",
    "new_map = gis.map(\"North Carolina\")\n",
    "#new_map.center\n",
    "new_map.zoom = 6\n",
    "new_map.basemap = 'gray-vector'\n",
    "house_sdf.spatial.plot(map_widget = new_map, renderer_type = 's', markersize = 3, symbol_type = 'simple')\n",
    "#new_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
